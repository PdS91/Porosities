{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use tandem neural Networks to solve the problem.\n",
    "Similar architectures to those used in the VAE, here adopted in a deterministic way. In a first step a forward network is trained to identify the density associated to a structure. In a second step the forward network is frozen and a backward network is trained to identify a microstructure associated to a density value in a deterministic way.\n",
    "\n",
    "This is a one to one relationship. A pragmatic approach to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1741770953211,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "M-BI1EkZVGld"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'c:/Users/Pietro/Desktop/Porosities/Porosities/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.Data import PorosityDistribution, extract_microstructures\n",
    "from Lib.Datasets import  MicrostructureDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.getcwd()+'/Job_Assignment_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = extract_microstructures(sample_path,keep_density_doubles=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data[77].plot_porosity_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9175,
     "status": "ok",
     "timestamp": 1741770967018,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "fXWLTsoGk5In"
   },
   "outputs": [],
   "source": [
    "# Create train, validation, and test datasets\n",
    "train_dataset = MicrostructureDataset(sample_path, train=True, val=False, test=False,device=device,keep_doubles=True)\n",
    "val_dataset = MicrostructureDataset(sample_path, train=False, val=True, test=False,device=device,keep_doubles=True)\n",
    "test_dataset = MicrostructureDataset(sample_path, train=False, val=False, test=True,device=device,keep_doubles=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1741770967052,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "xw-JG-JCm1g2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Forward(nn.Module):\n",
    "    def __init__(self,scale=1):\n",
    "        super(Forward, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        # 3D Convolutional Layers with Batch Normalization\n",
    "        self.conv1 = nn.Conv3d(1, 4*scale, kernel_size=3, stride=2) #Bx8x14x14x14\n",
    "        self.bn1 = nn.BatchNorm3d(4*scale)  # Batch Normalization after conv1\n",
    "        self.conv2 = nn.Conv3d(4*scale, 8*scale, kernel_size=3, stride=2) #Bx16x6x6x6\n",
    "        self.bn2 = nn.BatchNorm3d(8*scale)  # Batch Normalization after conv2\n",
    "        self.conv3 = nn.Conv3d(8*scale, 16*scale, kernel_size=3, stride=2) #Bx32x2x2x2\n",
    "        self.bn3 = nn.BatchNorm3d(16*scale)  # Batch Normalization after conv3\n",
    "        self.conv4 = nn.Conv3d(16*scale, 32*scale, kernel_size=2, stride=2) #Bx64x1x1x1\n",
    "        self.bn4 = nn.BatchNorm3d(32*scale)  # Batch Normalization after conv3\n",
    "\n",
    "        # Linear Layers with Dropout\n",
    "        self.fc1 = nn.Linear(32*scale, 16*scale)\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout after fc1\n",
    "        self.fc2 = nn.Linear(16*scale, 8*scale)\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout after fc2\n",
    "        self.fc3 = nn.Linear(8*scale, 4*scale)\n",
    "        self.dropout3 = nn.Dropout(0.1)  # Dropout after fc3\n",
    "        self.regressor = nn.Linear(4*scale,1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.Silu = nn.SiLU()\n",
    "\n",
    "    def convolution(self,x):\n",
    "        x = self.Silu(self.bn1(self.conv1(x)))\n",
    "        x = self.Silu(self.bn2(self.conv2(x)))\n",
    "        x = self.Silu(self.bn3(self.conv3(x)))\n",
    "        x = self.Silu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def nl_projection(self,x):\n",
    "        x = self.Silu(self.dropout1(self.fc1(x)))\n",
    "        x = self.Silu(self.dropout2(self.fc2(x)))\n",
    "        x = self.Silu(self.dropout2(self.fc3(x)))\n",
    "\n",
    "        return self.regressor(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x)\n",
    "        x = x.view(-1, 32 * self.scale)  # Flatten for linear layers\n",
    "        x = self.nl_projection(x)\n",
    "\n",
    "        return x.squeeze()\n",
    "    \n",
    "    \n",
    "class Backward(nn.Module):\n",
    "    def __init__(self,scale=1):\n",
    "        super(Backward, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        # Linear Layers with Dropout\n",
    "        self.fc1 = nn.Linear(1,4*scale)\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout after fc1\n",
    "        self.fc2 = nn.Linear(4*scale, 8*scale)\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout after fc2\n",
    "        self.fc3 = nn.Linear(8*scale, 16*scale)\n",
    "        self.dropout3 = nn.Dropout(0.1)  # Dropout after fc3\n",
    "        self.fc4 = nn.Linear(16*scale,32*scale)\n",
    "        self.dropout4 = nn.Dropout(0.1)  # Dropout after fc3\n",
    "        self.regressor = nn.Linear(32*scale,30*30*30)\n",
    "\n",
    "        # Activation Function\n",
    "        self.Silu = nn.SiLU()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Silu(self.dropout1(self.fc1(x)))\n",
    "        x = self.Silu(self.dropout2(self.fc2(x)))\n",
    "        x = self.Silu(self.dropout2(self.fc3(x)))\n",
    "        x = self.Silu(self.dropout2(self.fc4(x)))\n",
    "        x = self.regressor(x)\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x.view(-1,30,30,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1741770967820,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "vvWVSQQVnPbV",
    "outputId": "51b6378d-8f1a-4914-dda3-0e6cd29c65b4"
   },
   "outputs": [],
   "source": [
    "(X,y) = next(iter(train_dataloader))\n",
    "forward = Forward(scale=4)\n",
    "forward.to(device)\n",
    "forward(X[:,3,:,:,:].unsqueeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967841,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "btKGUE29ybVB"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer\n",
    "\n",
    "optimizer = optim.Adam(forward.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967844,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "AM8V5LWozGgS"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 350\n",
    "scale = 0.05\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    forward.train()  # Set the model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs = inputs[:,3,:,:,:].unsqueeze(1)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = forward(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)/scale  # Calculate the loss\n",
    "        loss.backward()  # Backpropagate the gradients\n",
    "        optimizer.step()  # Update the model's weights\n",
    "        running_train_loss += loss.item()\n",
    "    epoch_train_loss = running_train_loss / len(train_dataloader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    forward.eval()  # Set the model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for i, (inputs, targets) in enumerate(val_dataloader):\n",
    "            inputs = inputs[:,3,:,:,:].unsqueeze(1)\n",
    "            outputs = forward(inputs)\n",
    "            loss = criterion(outputs, targets)/scale\n",
    "            running_val_loss += loss.item()\n",
    "    epoch_val_loss = running_val_loss / len(val_dataloader)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {epoch_train_loss:.4f} Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the test dataset\n",
    "forward.eval()  # Set the model to evaluation mode\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, targets in test_dataloader:\n",
    "        predictions = forward(inputs[:,3,:,:,:].unsqueeze(1))\n",
    "        all_predictions.extend(predictions.tolist())\n",
    "        all_targets.extend(targets.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=all_targets, y=all_predictions,\n",
    "                 opacity=0.5,  # Set opacity for better visualization\n",
    "                 trendline=\"ols\",  # Add trendline using Ordinary Least Squares\n",
    "                 title=\"Regression Plot: Actual vs. Predicted Density\")\n",
    "\n",
    "# Add ideal line (y = x)\n",
    "x_range = np.linspace(min(all_targets), max(all_targets), 100)\n",
    "fig.add_scatter(x=x_range, y=x_range, mode='lines',\n",
    "                line=dict(color='red', dash='dash'),\n",
    "                name='Ideal Line (y = x)')\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Actual Density\",\n",
    "    yaxis_title=\"Predicted Density\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions for the test dataset\n",
    "forward.eval()  # Set the model to evaluation mode\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for inputs, targets in train_dataloader:\n",
    "        predictions = forward(inputs[:,3,:,:,:].unsqueeze(1))\n",
    "        all_predictions.extend(predictions.tolist())\n",
    "        all_targets.extend(targets.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=all_targets, y=all_predictions,\n",
    "                 opacity=0.5,  # Set opacity for better visualization\n",
    "                 trendline=\"ols\",  # Add trendline using Ordinary Least Squares\n",
    "                 title=\"Regression Plot: Actual vs. Predicted Density\")\n",
    "\n",
    "# Add ideal line (y = x)\n",
    "x_range = np.linspace(min(all_targets), max(all_targets), 100)\n",
    "fig.add_scatter(x=x_range, y=x_range, mode='lines',\n",
    "                line=dict(color='red', dash='dash'),\n",
    "                name='Ideal Line (y = x)')\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Actual Density\",\n",
    "    yaxis_title=\"Predicted Density\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward = Backward(scale=4).to(device)\n",
    "\n",
    "for param in forward.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "forward.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_criterion = torch.nn.BCELoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(backward.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "alpha = 0.5\n",
    "stable_reg = 1\n",
    "stable_loss = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        backward.train()\n",
    "        inputs = inputs[:,3,:,:,:].unsqueeze(1)\n",
    "        rec_structure = backward(targets.unsqueeze(-1))\n",
    "        rec_structure = rec_structure.unsqueeze(1)\n",
    "        reg_loss = reg_criterion(rec_structure,inputs)\n",
    "        loss = criterion(forward(rec_structure),targets)\n",
    "        tot_loss = (1-alpha)*reg_loss/stable_reg+alpha*loss/stable_loss\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        tot_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Train: Epoch [{epoch+1}/{num_epochs}], Reg: {reg_loss.item():.4f}, Loss: {loss.item():.4f}, Tot_Loss: {tot_loss.item():.4f}')\n",
    "        \n",
    "        backward.eval()\n",
    "        for inputs, targets in test_dataloader:\n",
    "            inputs = inputs[:,3,:,:,:].unsqueeze(1)\n",
    "            rec_structure = backward(targets.unsqueeze(-1))\n",
    "            rec_structure = rec_structure.unsqueeze(1)\n",
    "            reg_loss = reg_criterion(rec_structure,inputs)\n",
    "            loss = criterion(forward(rec_structure),targets)\n",
    "            tot_loss = (1-alpha)*reg_loss/stable_reg+alpha*loss/stable_loss\n",
    "        \n",
    "        print(f'Validation: Epoch [{epoch+1}/{num_epochs}], Reg: {reg_loss.item():.4f}, Loss: {loss.item():.4f}, Tot_Loss: {tot_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = extracted_data[0]\n",
    "original_data.plot_porosity_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(backward.parameters()).device\n",
    "micro = original_data.as_tensor().to(device)\n",
    "micro = micro.permute(3,0,1,2)\n",
    "micro_rec = micro\n",
    "X,y = micro[3,:,:,:].unsqueeze(0),micro[4,0,0,0].unsqueeze(dim=0)\n",
    "print(micro.shape)\n",
    "X_rec = backward(y)\n",
    "micro_rec[3,:,:,:] = X_rec.squeeze()\n",
    "micro_rec = micro_rec.permute(1,2,3,0).reshape(30*30*30,-1)\n",
    "micro_rec = micro_rec.detach().cpu().numpy()\n",
    "print(micro_rec.shape)\n",
    "\n",
    "rec_data = PorosityDistribution(micro_rec[:,:-1],y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data.plot_porosity_distribution(porosity=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro[4,0,0,0].unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPu6mAXAX36tAri1fom+JvV",
   "provenance": [
    {
     "file_id": "1Kc2aZZvLQha0Y4R3eNMnC0oT6dTpNpIz",
     "timestamp": 1741691477194
    }
   ]
  },
  "kernelspec": {
   "display_name": "pds_standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
