{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1741770953211,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "M-BI1EkZVGld"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/gpfs/data/ssa/users/d602145/Workspace/scratch/Porosity/ETH/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.Data import PorosityDistribution, extract_microstructures\n",
    "from Lib.Datasets import  PorosityDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.getcwd()+'/Job_Assignment_Data/Job_Assignment_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9175,
     "status": "ok",
     "timestamp": 1741770967018,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "fXWLTsoGk5In"
   },
   "outputs": [],
   "source": [
    "# Create train, validation, and test datasets\n",
    "train_dataset = PorosityDataset(sample_path, train=True, val=False, test=False,keep_doubles=False,device=device)\n",
    "val_dataset = PorosityDataset(sample_path, train=False, val=True, test=False,keep_doubles=False,device=device)\n",
    "test_dataset = PorosityDataset(sample_path, train=False, val=False, test=True,keep_doubles=False,device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1280, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1280, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1280, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualModuleBlock(nn.Module):\n",
    "    def __init__(self, dim, steps, dropout=0.1, residual=False, batch_norm=True):\n",
    "        super(ResidualModuleBlock,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.residual = residual\n",
    "        \n",
    "        for i in steps:\n",
    "            self.layers.append(nn.Linear(dim, dim))\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(dim))\n",
    "            self.layers.append(nn.SiLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "        def forward(self,x):\n",
    "            residual = x\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "            if residual:\n",
    "                x += residual\n",
    "            return x\n",
    "        \n",
    "class LinearModuleBlock(nn.Module):\n",
    "    def __init__(self, dims, dropout=0.1, batch_norm=True):\n",
    "        super(ResidualModuleBlock,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(dims)-1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            self.layers.append(nn.SiLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "        def forward(self,x):\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1741770967052,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "xw-JG-JCm1g2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,scale=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        # Density Condition\n",
    "        self.fc1c = nn.Sequential(nn.Linear(1, 8 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "        self.fc2c = nn.Sequential(nn.Linear(1, 16 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "\n",
    "        # Linear Layers with Dropout\n",
    "        self.fc1 = nn.Linear(3, 8*scale)\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout after fc1\n",
    "        self.fc2 = nn.Linear(8*scale, 16*scale)\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout after fc2\n",
    "        self.fc3 = nn.Linear(16*scale, 32*scale)\n",
    "        self.fc1 = nn.Linear(3, 8*scale)\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout after fc1\n",
    "        self.fc2 = nn.Linear(8*scale, 16*scale)\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout after fc2\n",
    "        self.fc3 = nn.Linear(16*scale, 32*scale)\n",
    "\n",
    "        # Activation Function\n",
    "        self.Silu = nn.SiLU()\n",
    "\n",
    "    def nl_projection(self,x,y):\n",
    "        x = self.Silu(self.dropout1(self.fc1(x)))\n",
    "        x = x + self.fc1c(y.view(-1, 1))\n",
    "        x = self.Silu(self.dropout2(self.fc2(x)))\n",
    "        x = x + self.fc2c(y.view(-1, 1))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.nl_projection(x,y)\n",
    "\n",
    "        return x.squeeze()\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, scale=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        # Density Condition\n",
    "        self.fc1c = nn.Sequential(nn.Linear(1, 16 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "        self.fc2c = nn.Sequential(nn.Linear(1, 8 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "\n",
    "        # Linear Layers with Dropout (mirroring encoder)\n",
    "\n",
    "        #input Bx8\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * scale, 16 * scale)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(16 * scale, 8 * scale)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.fc3 = nn.Linear(8 * scale, 3)\n",
    "\n",
    "\n",
    "        # Activation Function\n",
    "        self.Silu = nn.SiLU()\n",
    "\n",
    "    def nl_projection(self, x, y):\n",
    "        x = self.Silu(self.dropout1(self.fc1(x)))\n",
    "        x = x + self.fc1c(y.view(-1, 1))\n",
    "        x = self.Silu(self.dropout2(self.fc2(x)))\n",
    "        x = x + self.fc2c(y.view(-1, 1))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        x = self.nl_projection(x,y)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741770967068,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "FwcOa_eYIOnk"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConditionedVAE(nn.Module):\n",
    "    def __init__(self, scale=1):\n",
    "        super(ConditionedVAE, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.encoder = Encoder(scale=scale)  # Your existing Encoder\n",
    "        self.decoder = Decoder(scale=scale)  # Your existing Decoder\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        # Add layers for mean and variance of the latent space\n",
    "        self.fc_mu = nn.Linear(32 * scale, 32 * scale)  # Output dimension for mean\n",
    "        self.fc_logvar = nn.Linear(32 * scale, 32 * scale)  # Output dimension for log variance\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick to sample from the latent space.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        # Encode the input\n",
    "        h = self.encoder(x, y)\n",
    "\n",
    "        # Get mean and log variance\n",
    "        mu = self.fc_mu(self.dropout1(h))\n",
    "        logvar = self.fc_logvar(self.dropout2(h))\n",
    "\n",
    "        # Sample from the latent space\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode the latent representation\n",
    "        x_recon = self.decoder(z, y)\n",
    "\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "    def sample(self,num_samples,density,device):\n",
    "        \n",
    "        z = torch.randn(num_samples,32*self.scale).to(device)\n",
    "        y = density*torch.ones(num_samples,1).to(device)\n",
    "        samples = self.decoder(z,y)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,y) = next(iter(train_dataloader))\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((X,y.view(-1,1)),dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0],y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1741770967820,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "vvWVSQQVnPbV",
    "outputId": "51b6378d-8f1a-4914-dda3-0e6cd29c65b4"
   },
   "outputs": [],
   "source": [
    "(X,y) = next(iter(train_dataloader))\n",
    "model = ConditionedVAE(scale=4)\n",
    "model.to(device)\n",
    "model(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967841,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "btKGUE29ybVB"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967844,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "AM8V5LWozGgS"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion_reconstruction = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_2GzZKgzJeE",
    "outputId": "85d94d24-d85d-47f3-fc82-53375c6ea5bf"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses = []\n",
    "train_recon_losses = []\n",
    "train_cond_losses = []\n",
    "train_kl_losses = []\n",
    "val_losses = []\n",
    "val_recon_losses = []\n",
    "val_cond_losses = []\n",
    "val_kl_losses = []\n",
    "beta = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_recon_loss = 0.0\n",
    "    running_train_kl_loss = 0.0\n",
    "\n",
    "    for i, (inputs, conditions) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs, mu, logvar = model(inputs, conditions)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # Calculate individual losses\n",
    "        loss_reconstruction = criterion_reconstruction(outputs, inputs)\n",
    "\n",
    "        # Combine losses with weights (adjust as needed)\n",
    "        loss = loss_reconstruction + beta*kl_loss # Example: 0.1 weight for condition loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        running_train_recon_loss += loss_reconstruction.item()\n",
    "        running_train_kl_loss += kl_loss.item()\n",
    "\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_dataloader)\n",
    "    epoch_train_recon_loss = running_train_recon_loss / len(train_dataloader)\n",
    "    epoch_train_kl_loss = running_train_kl_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_recon_loss = 0.0\n",
    "    running_val_kl_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, conditions) in enumerate(val_dataloader):\n",
    "            \n",
    "            outputs, mu, logvar = model(inputs, conditions)\n",
    "\n",
    "            # Calculate individual losses\n",
    "            loss_reconstruction = criterion_reconstruction(outputs, inputs)\n",
    "            \n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            # Combine losses with weights (adjust as needed)\n",
    "            loss = loss_reconstruction + beta*kl_loss # Example: 0.1 weight for condition loss\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            running_val_recon_loss += loss_reconstruction.item()\n",
    "            running_val_kl_loss += kl_loss.item()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_dataloader)\n",
    "    epoch_val_recon_loss = running_val_recon_loss / len(val_dataloader)\n",
    "\n",
    "    epoch_val_kl_loss = running_val_kl_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} \"\n",
    "          f\"Train Reconstruction Loss: {epoch_train_recon_loss:.4f} \"\n",
    "          f\"Train KL Loss: {epoch_train_kl_loss:.4f} \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f} \"\n",
    "          f\"Val Reconstruction Loss: {epoch_val_recon_loss:.4f} \"\n",
    "          f\"Val KL Loss: {epoch_val_kl_loss:.4f} \")\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_recon_losses.append(epoch_train_recon_loss)\n",
    "    train_kl_losses.append(epoch_train_kl_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_recon_losses.append(epoch_val_recon_loss)\n",
    "    val_kl_losses.append(epoch_val_kl_loss)\n",
    "\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfwxPPOT4kiU"
   },
   "outputs": [],
   "source": [
    "# Plotting (after the training loop)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Total loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Total Loss')\n",
    "\n",
    "# Individual losses\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_recon_losses, label='Train Reconstruction Loss')\n",
    "plt.plot(train_cond_losses, label='Train Condition Loss')\n",
    "plt.plot(val_recon_losses, label='Val Reconstruction Loss')\n",
    "plt.plot(val_cond_losses, label='Val Condition Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Individual Losses')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 500\n",
    "density = 0.1\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGLOdL5vd_TP"
   },
   "outputs": [],
   "source": [
    "\n",
    "rec = model.sample(samples,density,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rec.detach().to('cpu').numpy(),columns=['x','y','z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(df,x='x',y='y',z='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df,facet_col='variable',histnorm='probability',nbins=100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPu6mAXAX36tAri1fom+JvV",
   "provenance": [
    {
     "file_id": "1Kc2aZZvLQha0Y4R3eNMnC0oT6dTpNpIz",
     "timestamp": 1741691477194
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
