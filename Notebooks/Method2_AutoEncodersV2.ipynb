{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1741770953211,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "M-BI1EkZVGld"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/gpfs/data/ssa/users/d602145/Workspace/scratch/Porosity/ETH/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.Data import PorosityDistribution, extract_microstructures\n",
    "from Lib.Datasets import  PorosityDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.getcwd()+'/Job_Assignment_Data/Job_Assignment_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9175,
     "status": "ok",
     "timestamp": 1741770967018,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "fXWLTsoGk5In"
   },
   "outputs": [],
   "source": [
    "# Create train, validation, and test datasets\n",
    "train_dataset = PorosityDataset(sample_path, train=True, val=False, test=False,keep_doubles=False,device=device)\n",
    "val_dataset = PorosityDataset(sample_path, train=False, val=True, test=False,keep_doubles=False,device=device)\n",
    "test_dataset = PorosityDataset(sample_path, train=False, val=False, test=True,keep_doubles=False,device=device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1280, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1280, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1280, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualModuleBlock(nn.Module):\n",
    "    def __init__(self, dim, steps, dropout=0.1, residual=False, batch_norm=True):\n",
    "        super(ResidualModuleBlock,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.residual = residual\n",
    "        \n",
    "        for i in range(steps):\n",
    "            self.layers.append(nn.Linear(dim, dim))\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(dim))\n",
    "            self.layers.append(nn.SiLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        if self.residual:\n",
    "            x += residual\n",
    "        return x\n",
    "        \n",
    "class LinearModuleBlock(nn.Module):\n",
    "    def __init__(self, dims, dropout=0.1, batch_norm=True):\n",
    "        super(LinearModuleBlock,self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(dims)-1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(dims[i+1]))\n",
    "            self.layers.append(nn.SiLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1741770967052,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "xw-JG-JCm1g2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,scale=1,condition_dim=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        # Linear Layers with Dropout\n",
    "        \n",
    "        self.input_block = LinearModuleBlock([3+condition_dim,scale*8,scale*16])\n",
    "        self.deep_block = ResidualModuleBlock(scale*16,2,residual=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_block(x)\n",
    "        x = self.deep_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, scale=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.scale = scale\n",
    "        # Linear Layers with Dropout\n",
    "        \n",
    "        self.linproj = nn.Linear(scale*8,3)\n",
    "        self.output_block = LinearModuleBlock([scale*16,scale*8])\n",
    "        self.deep_block = ResidualModuleBlock(scale*16,4,residual=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.deep_block(x)\n",
    "        x = self.output_block(x)\n",
    "        \n",
    "        return self.linproj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,y) = next(iter(train_dataloader))\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = encoder(X)\n",
    "out = decoder(hidden)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741770967068,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "FwcOa_eYIOnk"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConditionedVAE(nn.Module):\n",
    "    def __init__(self, scale=1):\n",
    "        super(ConditionedVAE, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.encoder = Encoder(scale=scale)\n",
    "        self.decoder = Decoder(scale=scale)\n",
    "        \n",
    "        self.condition_encoder = LinearModuleBlock([1,scale*8,scale*16])\n",
    "        # Add layers for mean and variance of the latent space\n",
    "        self.fc_mu =nn.Linear(scale*16,scale*16)\n",
    "        self.fc_logvar = nn.Linear(scale*16,scale*16)  # Output dimension for log variance\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick to sample from the latent space.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        # Encode the input\n",
    "        h = self.encoder(x)\n",
    "\n",
    "        # Get mean and log variance\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "\n",
    "        # Sample from the latent space\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Add Conditioning\n",
    "        \n",
    "        \n",
    "        z += z + self.condition_encoder(y.view(-1,1))\n",
    "        \n",
    "        # Decode the latent representation\n",
    "        x_recon = self.decoder(z)\n",
    "\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "    def sample(self,num_samples,density,device):\n",
    "        \n",
    "        z = torch.randn(num_samples,16*self.scale).to(device)\n",
    "        y = density*torch.ones(num_samples,1).to(device)\n",
    "        \n",
    "        z += z + self.condition_encoder(y)\n",
    "        samples = self.decoder(z)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConditionedVAE2(nn.Module):\n",
    "    def __init__(self, scale=1):\n",
    "        super(ConditionedVAE2, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.encoder = Encoder(scale=scale)\n",
    "        self.decoder = Decoder(scale=2*scale)\n",
    "        \n",
    "        self.condition_encoder = LinearModuleBlock([1,8,8])\n",
    "        self.condition_decoder = LinearModuleBlock([1,scale*8,scale*16])\n",
    "        # Add layers for mean and variance of the latent space\n",
    "        self.fc_mu =nn.Linear(scale*16,scale*16)\n",
    "        self.fc_logvar = nn.Linear(scale*16,scale*16)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick to sample from the latent space.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        # Encode the input\n",
    "        \n",
    "        h = self.encoder(x)\n",
    "\n",
    "        # Get mean and log variance\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "\n",
    "        # Sample from the latent space\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Add Conditioning\n",
    "        \n",
    "        \n",
    "        z = torch.cat((z,self.condition_decoder(y.view(-1,1))),dim=-1)\n",
    "        \n",
    "        # Decode the latent representation\n",
    "        x_recon = self.decoder(z)\n",
    "\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "    def sample(self,num_samples,density,device):\n",
    "        \n",
    "        z = torch.randn(num_samples,16*self.scale).to(device)\n",
    "        y = density*torch.ones(num_samples,1).to(device)\n",
    "        \n",
    "        z  = torch.cat((z,self.condition_decoder(y)),dim=-1)\n",
    "        samples = self.decoder(z)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,y) = next(iter(train_dataloader))\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((X,y.view(-1,1)),dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0],y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1741770967820,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "vvWVSQQVnPbV",
    "outputId": "51b6378d-8f1a-4914-dda3-0e6cd29c65b4"
   },
   "outputs": [],
   "source": [
    "(X,y) = next(iter(train_dataloader))\n",
    "model = ConditionedVAE2(scale=4)\n",
    "model.to(device)\n",
    "model(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967841,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "btKGUE29ybVB"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967844,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "AM8V5LWozGgS"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion_reconstruction = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_2GzZKgzJeE",
    "outputId": "85d94d24-d85d-47f3-fc82-53375c6ea5bf"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "train_losses = []\n",
    "train_recon_losses = []\n",
    "train_cond_losses = []\n",
    "train_kl_losses = []\n",
    "val_losses = []\n",
    "val_recon_losses = []\n",
    "val_cond_losses = []\n",
    "val_kl_losses = []\n",
    "beta = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_recon_loss = 0.0\n",
    "    running_train_kl_loss = 0.0\n",
    "\n",
    "    for i, (inputs, conditions) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, mu, logvar = model(inputs, conditions)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # Calculate individual losses\n",
    "        loss_reconstruction = criterion_reconstruction(outputs, inputs)\n",
    "\n",
    "        # Combine losses with weights (adjust as needed)\n",
    "        loss = loss_reconstruction + beta*kl_loss # Example: 0.1 weight for condition loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        running_train_recon_loss += loss_reconstruction.item()\n",
    "        running_train_kl_loss += kl_loss.item()\n",
    "\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_dataloader)\n",
    "    epoch_train_recon_loss = running_train_recon_loss / len(train_dataloader)\n",
    "    epoch_train_kl_loss = running_train_kl_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_recon_loss = 0.0\n",
    "    running_val_kl_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, conditions) in enumerate(val_dataloader):\n",
    "            \n",
    "            outputs, mu, logvar = model(inputs, conditions)\n",
    "\n",
    "            # Calculate individual losses\n",
    "            loss_reconstruction = criterion_reconstruction(outputs, inputs)\n",
    "            \n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            # Combine losses with weights (adjust as needed)\n",
    "            loss = loss_reconstruction + beta*kl_loss # Example: 0.1 weight for condition loss\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            running_val_recon_loss += loss_reconstruction.item()\n",
    "            running_val_kl_loss += kl_loss.item()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_dataloader)\n",
    "    epoch_val_recon_loss = running_val_recon_loss / len(val_dataloader)\n",
    "\n",
    "    epoch_val_kl_loss = running_val_kl_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} \"\n",
    "          f\"Train Reconstruction Loss: {epoch_train_recon_loss:.4f} \"\n",
    "          f\"Train KL Loss: {epoch_train_kl_loss:.4f} \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f} \"\n",
    "          f\"Val Reconstruction Loss: {epoch_val_recon_loss:.4f} \"\n",
    "          f\"Val KL Loss: {epoch_val_kl_loss:.4f} \")\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_recon_losses.append(epoch_train_recon_loss)\n",
    "    train_kl_losses.append(epoch_train_kl_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_recon_losses.append(epoch_val_recon_loss)\n",
    "    val_kl_losses.append(epoch_val_kl_loss)\n",
    "\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfwxPPOT4kiU"
   },
   "outputs": [],
   "source": [
    "# Plotting (after the training loop)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Total loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Total Loss')\n",
    "\n",
    "# Individual losses\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_recon_losses, label='Train Reconstruction Loss')\n",
    "plt.plot(train_cond_losses, label='Train Condition Loss')\n",
    "plt.plot(val_recon_losses, label='Val Reconstruction Loss')\n",
    "plt.plot(val_cond_losses, label='Val Condition Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Individual Losses')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 500\n",
    "density = 0.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGLOdL5vd_TP"
   },
   "outputs": [],
   "source": [
    "\n",
    "samples = 500\n",
    "density = 0.683\n",
    "model.eval()\n",
    "\n",
    "rec = model.sample(samples,density,device=device)\n",
    "df = pd.DataFrame(rec.detach().to('cpu').numpy(),columns=['x','y','z'])\n",
    "fig = px.scatter_3d(df,x='x',y='y',z='z')\n",
    "fig.show()\n",
    "fig = px.histogram(df,facet_col='variable',histnorm='probability',nbins=100)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = 500\n",
    "density = 0.5\n",
    "model.eval()\n",
    "\n",
    "rec = model.sample(samples,density,device=device)\n",
    "df = pd.DataFrame(rec.detach().to('cpu').numpy(),columns=['x','y','z'])\n",
    "fig = px.scatter_3d(df,x='x',y='y',z='z')\n",
    "fig.show()\n",
    "fig = px.histogram(df,facet_col='variable',histnorm='probability',nbins=100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 500\n",
    "density = 0.8\n",
    "model.eval()\n",
    "\n",
    "rec = model.sample(samples,density,device=device)\n",
    "df = pd.DataFrame(rec.detach().to('cpu').numpy(),columns=['x','y','z'])\n",
    "fig = px.scatter_3d(df,x='x',y='y',z='z')\n",
    "fig.show()\n",
    "fig = px.histogram(df,facet_col='variable',histnorm='probability',nbins=100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPu6mAXAX36tAri1fom+JvV",
   "provenance": [
    {
     "file_id": "1Kc2aZZvLQha0Y4R3eNMnC0oT6dTpNpIz",
     "timestamp": 1741691477194
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
