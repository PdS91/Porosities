{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1741770953211,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "M-BI1EkZVGld"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/gpfs/data/ssa/users/d602145/Workspace/scratch/Porosity/ETH/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.Data import PorosityDistribution, Extract_data\n",
    "from Lib.Datasets import  PorosityDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.getcwd()+'/Job_Assignment_Data/Job_Assignment_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = Extract_data(sample_path,keep_density_doubles=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data[77].plot_porosity_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9175,
     "status": "ok",
     "timestamp": 1741770967018,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "fXWLTsoGk5In"
   },
   "outputs": [],
   "source": [
    "# Create train, validation, and test datasets\n",
    "train_dataset = PorosityDataset(sample_path, train=True, val=False, test=False,keep_doubles=False)\n",
    "val_dataset = PorosityDataset(sample_path, train=False, val=True, test=False,keep_doubles=False)\n",
    "test_dataset = PorosityDataset(sample_path, train=False, val=False, test=True,keep_doubles=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1741770967052,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "xw-JG-JCm1g2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,scale=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        # Density Condition\n",
    "        self.fc1c = nn.Sequential(nn.Linear(1, 32 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "        self.fc2c = nn.Sequential(nn.Linear(1, 16 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "\n",
    "\n",
    "        # 3D Convolutional Layers with Batch Normalization\n",
    "        self.conv1 = nn.Conv3d(1, 8*scale, kernel_size=3, stride=2) #Bx8x14x14x14\n",
    "        self.bn1 = nn.BatchNorm3d(8*scale)  # Batch Normalization after conv1\n",
    "        self.conv2 = nn.Conv3d(8*scale, 16*scale, kernel_size=3, stride=2) #Bx16x6x6x6\n",
    "        self.bn2 = nn.BatchNorm3d(16*scale)  # Batch Normalization after conv2\n",
    "        self.conv3 = nn.Conv3d(16*scale, 32*scale, kernel_size=3, stride=2) #Bx32x2x2x2\n",
    "        self.bn3 = nn.BatchNorm3d(32*scale)  # Batch Normalization after conv3\n",
    "        self.conv4 = nn.Conv3d(32*scale, 64*scale, kernel_size=2, stride=2) #Bx64x1x1x1\n",
    "        self.bn4 = nn.BatchNorm3d(64*scale)  # Batch Normalization after conv3\n",
    "\n",
    "        # Linear Layers with Dropout\n",
    "        self.fc1 = nn.Linear(64*scale, 32*scale)\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout after fc1\n",
    "        self.fc2 = nn.Linear(32*scale, 16*scale)\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout after fc2\n",
    "        self.fc3 = nn.Linear(16*scale, 8*scale)\n",
    "\n",
    "        # Activation Function\n",
    "        self.Silu = nn.SiLU()\n",
    "\n",
    "    def convolution(self,x):\n",
    "        x = self.Silu(self.bn1(self.conv1(x)))\n",
    "        x = self.Silu(self.bn2(self.conv2(x)))\n",
    "        x = self.Silu(self.bn3(self.conv3(x)))\n",
    "        x = self.Silu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def nl_projection(self,x,y):\n",
    "        x = self.Silu(self.dropout1(self.fc1(x)))\n",
    "        x = x + self.fc1c(y.view(-1, 1))\n",
    "        x = self.Silu(self.dropout2(self.fc2(x)))\n",
    "        x = x + self.fc2c(y.view(-1, 1))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.convolution(x)\n",
    "        x = x.view(-1, 64 * self.scale)  # Flatten for linear layers\n",
    "        x = self.nl_projection(x,y)\n",
    "\n",
    "        return x.squeeze()\n",
    "\n",
    "class Decoder2(nn.Module):\n",
    "    def __init__(self, scale=1):\n",
    "        super(Decoder2, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        # Density Condition\n",
    "        self.fc1c = nn.Sequential(nn.Linear(1, 16 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "        self.fc2c = nn.Sequential(nn.Linear(1, 32 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "\n",
    "        # Linear Layers with Dropout (mirroring encoder)\n",
    "\n",
    "        #input Bx8\n",
    "\n",
    "        self.fc1 = nn.Linear(8 * scale, 16 * scale)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(16 * scale, 32 * scale)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.fc3 = nn.Linear(32 * scale, 64 * scale)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        self.fc4 = nn.Linear(64 * scale, 27000)\n",
    "\n",
    "        # 3D Convolutional Transpose Layers with Batch Normalization\n",
    "\n",
    "        #input: Bx64x1x1x1\n",
    "\n",
    "        # Activation Function\n",
    "        self.Silu = nn.SiLU()\n",
    "\n",
    "    def nl_projection(self, x, y):\n",
    "        x = self.Silu(self.dropout1(self.fc1(x)))\n",
    "        x = x + self.fc1c(y.view(-1, 1))\n",
    "        x = self.Silu(self.dropout2(self.fc2(x)))\n",
    "        x = x + self.fc2c(y.view(-1, 1))\n",
    "        x = self.Silu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.nl_projection(x,y)\n",
    "        x = x.view(-1, 1, 30, 30, 30)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741770967059,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "68TnKQBSgiSi"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, scale=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "        # Density Condition\n",
    "        self.fc1c = nn.Sequential(nn.Linear(1, 16 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "        self.fc2c = nn.Sequential(nn.Linear(1, 32 * scale), nn.Dropout(0.1), nn.SiLU())\n",
    "\n",
    "        # Linear Layers with Dropout (mirroring encoder)\n",
    "\n",
    "        #input Bx8\n",
    "\n",
    "        self.fc1 = nn.Linear(8 * scale, 16 * scale)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(16 * scale, 32 * scale)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.fc3 = nn.Linear(32 * scale, 64 * scale)\n",
    "\n",
    "        # 3D Convolutional Transpose Layers with Batch Normalization\n",
    "\n",
    "        #input: Bx64x1x1x1\n",
    "\n",
    "        self.convT1 = nn.ConvTranspose3d(64 * scale, 32 * scale, kernel_size=2, stride=1) #Bx32x2x2x2\n",
    "        self.bn1 = nn.BatchNorm3d(32 * scale)\n",
    "        self.convT2 = nn.ConvTranspose3d(32 * scale, 16 * scale, kernel_size=3, stride=2, output_padding=1) #Bx16x6x6x6\n",
    "        self.bn2 = nn.BatchNorm3d(16 * scale)\n",
    "        self.convT3 = nn.ConvTranspose3d(16 * scale, 8 * scale, kernel_size=4, stride=2, output_padding=1) #Bx8x15x15x15\n",
    "        self.bn3 = nn.BatchNorm3d(8 * scale)\n",
    "        self.convT4 = nn.ConvTranspose3d(8 * scale, 1, kernel_size=4, stride=2, padding=1) #Bx1x30x30x30\n",
    "\n",
    "        # Activation Function\n",
    "        self.Silu = nn.SiLU()\n",
    "\n",
    "    def nl_projection(self, x, y):\n",
    "        x = self.Silu(self.dropout1(self.fc1(x)))\n",
    "        x = x + self.fc1c(y.view(-1, 1))\n",
    "        x = self.Silu(self.dropout2(self.fc2(x)))\n",
    "        x = x + self.fc2c(y.view(-1, 1))\n",
    "        x = self.Silu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def convolution(self, x):\n",
    "        x = self.Silu(self.bn1(self.convT1(x)))\n",
    "        x = self.Silu(self.bn2(self.convT2(x)))\n",
    "        x = self.Silu(self.bn3(self.convT3(x)))\n",
    "        x = torch.sigmoid(self.convT4(x))  # Sigmoid activation for output\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.nl_projection(x,y)\n",
    "        x = x.view(-1, 64 * self.scale, 1, 1, 1)  # Reshape for convolutional layers\n",
    "        x = self.convolution(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967063,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "JB1IY62C9r0E"
   },
   "outputs": [],
   "source": [
    "class ConditionedAutoEncoder(nn.Module):\n",
    "    def __init__(self,scale=1):\n",
    "        super(ConditionedAutoEncoder, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.encoder = Encoder(scale=scale)\n",
    "        self.decoder = Decoder2(scale=scale)\n",
    "        self.regressor = nn.Linear(8*scale,1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        latent = self.encoder(x,y)\n",
    "        x = self.decoder(latent,y)\n",
    "        y_rec = self.regressor(latent)\n",
    "        return x,y_rec.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741770967068,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "FwcOa_eYIOnk"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConditionedVAE(nn.Module):\n",
    "    def __init__(self, scale=1):\n",
    "        super(ConditionedVAE, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.encoder = Encoder(scale=scale)  # Your existing Encoder\n",
    "        self.decoder = Decoder2(scale=scale)  # Your existing Decoder\n",
    "\n",
    "        # Add layers for mean and variance of the latent space\n",
    "        self.fc_mu = nn.Linear(8 * scale, 8 * scale)  # Output dimension for mean\n",
    "        self.fc_logvar = nn.Linear(8 * scale, 8 * scale)  # Output dimension for log variance\n",
    "        self.regressor = nn.Linear(8 * scale, 1)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick to sample from the latent space.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        # Encode the input\n",
    "        h = self.encoder(x, y)\n",
    "\n",
    "        # Get mean and log variance\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "\n",
    "        # Sample from the latent space\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode the latent representation\n",
    "        x_recon = self.decoder(z, y)\n",
    "        y_recon = self.regressor(z)\n",
    "\n",
    "        return x_recon, y_recon.squeeze(), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1741770967820,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "vvWVSQQVnPbV",
    "outputId": "51b6378d-8f1a-4914-dda3-0e6cd29c65b4"
   },
   "outputs": [],
   "source": [
    "(X,y) = next(iter(train_dataloader))\n",
    "model = ConditionedVAE(scale=8)\n",
    "model(X[:,3,:,:,:].unsqueeze(1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967841,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "btKGUE29ybVB"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741770967844,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "AM8V5LWozGgS"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion_reconstruction = nn.BCELoss()\n",
    "criterion_condition = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_2GzZKgzJeE",
    "outputId": "85d94d24-d85d-47f3-fc82-53375c6ea5bf"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "train_losses = []\n",
    "train_recon_losses = []\n",
    "train_cond_losses = []\n",
    "train_kl_losses = []\n",
    "val_losses = []\n",
    "val_recon_losses = []\n",
    "val_cond_losses = []\n",
    "val_kl_losses = []\n",
    "alpha = 0.9\n",
    "beta = 0.5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_recon_loss = 0.0\n",
    "    running_train_cond_loss = 0.0\n",
    "    running_train_recon_loss = 0.0\n",
    "    running_train_kl_loss = 0.0\n",
    "\n",
    "    for i, (inputs, conditions) in enumerate(train_dataloader):\n",
    "        inputs = inputs[:,3,:,:,:].unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, reconstructed_conditions, mu, logvar = model(inputs, conditions)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # Calculate individual losses\n",
    "        loss_reconstruction = criterion_reconstruction(outputs, inputs)\n",
    "        loss_condition = criterion_condition(reconstructed_conditions, conditions)\n",
    "\n",
    "        # Combine losses with weights (adjust as needed)\n",
    "        loss = alpha*(loss_reconstruction + beta*loss_condition) + (1-alpha)*kl_loss # Example: 0.1 weight for condition loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        running_train_recon_loss += loss_reconstruction.item()\n",
    "        running_train_cond_loss += loss_condition.item()\n",
    "        running_train_kl_loss += kl_loss.item()\n",
    "\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_dataloader)\n",
    "    epoch_train_recon_loss = running_train_recon_loss / len(train_dataloader)\n",
    "    epoch_train_cond_loss = running_train_cond_loss / len(train_dataloader)\n",
    "    epoch_train_kl_loss = running_train_kl_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_val_recon_loss = 0.0\n",
    "    running_val_cond_loss = 0.0\n",
    "    running_val_kl_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, conditions) in enumerate(val_dataloader):\n",
    "            inputs = inputs[:,3,:,:,:].unsqueeze(1)\n",
    "            outputs, reconstructed_conditions, mu, logvar = model(inputs, conditions)\n",
    "\n",
    "            # Calculate individual losses\n",
    "            loss_reconstruction = criterion_reconstruction(outputs, inputs)\n",
    "            loss_condition = criterion_condition(reconstructed_conditions, conditions)\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            # Combine losses with weights (adjust as needed)\n",
    "            loss = alpha*(loss_reconstruction + beta*loss_condition) + (1-alpha)*kl_loss # Example: 0.1 weight for condition loss\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            running_val_recon_loss += loss_reconstruction.item()\n",
    "            running_val_cond_loss += loss_condition.item()\n",
    "            running_val_kl_loss += kl_loss.item()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_dataloader)\n",
    "    epoch_val_recon_loss = running_val_recon_loss / len(val_dataloader)\n",
    "    epoch_val_cond_loss = running_val_cond_loss / len(val_dataloader)\n",
    "    epoch_val_kl_loss = running_val_kl_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} \"\n",
    "          f\"Train Reconstruction Loss: {epoch_train_recon_loss:.4f} \"\n",
    "          f\"Train Condition Loss: {epoch_train_cond_loss:.4f} \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f} \"\n",
    "          f\"Val Reconstruction Loss: {epoch_val_recon_loss:.4f} \"\n",
    "          f\"Val Condition Loss: {epoch_val_cond_loss:.4f}\")\n",
    "\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_recon_losses.append(epoch_train_recon_loss)\n",
    "    train_cond_losses.append(epoch_train_cond_loss)\n",
    "    train_kl_losses.append(epoch_train_kl_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_recon_losses.append(epoch_val_recon_loss)\n",
    "    val_cond_losses.append(epoch_val_cond_loss)\n",
    "    val_kl_losses.append(epoch_val_kl_loss)\n",
    "\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfwxPPOT4kiU"
   },
   "outputs": [],
   "source": [
    "# Plotting (after the training loop)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Total loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Total Loss')\n",
    "\n",
    "# Individual losses\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_recon_losses, label='Train Reconstruction Loss')\n",
    "plt.plot(train_cond_losses, label='Train Condition Loss')\n",
    "plt.plot(val_recon_losses, label='Val Reconstruction Loss')\n",
    "plt.plot(val_cond_losses, label='Val Condition Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Individual Losses')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-oyKwWvZY_X"
   },
   "outputs": [],
   "source": [
    "original_data = extracted_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTSX4bfmfWP9"
   },
   "outputs": [],
   "source": [
    "original_data.plot_porosity_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLRL93e_FFpk"
   },
   "outputs": [],
   "source": [
    "original_data.plot_porosity_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JAUCtq-G2op"
   },
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "micro = original_data.as_tensor().to(device)\n",
    "micro = micro.permute(3,0,1,2)\n",
    "micro_rec = micro\n",
    "X,y = micro[3,:,:,:].unsqueeze(0),micro[4,0,0,0]\n",
    "X_rec, y_rec, mu, logvar = model(X.unsqueeze(0),y)\n",
    "micro_rec[3,:,:,:] = X_rec.squeeze()\n",
    "micro_rec[4,:,:,:] = y_rec\n",
    "micro_rec = micro_rec.permute(1,2,3,0).reshape(30*30*30,-1)\n",
    "micro_rec = micro_rec.detach().cpu().numpy()\n",
    "print(micro_rec.shape)\n",
    "\n",
    "rec_data = PorosityDistribution(micro_rec[:,:-1],y_rec.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1741770835203,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "25NTw1NVbKAx",
    "outputId": "f85e0120-3c32-4be9-c36e-8e72b7747ef5"
   },
   "outputs": [],
   "source": [
    "original_data.distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1741770836021,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "GPFALxu7bW9r",
    "outputId": "19e8f51e-d861-4b06-e25a-a61033b79b66"
   },
   "outputs": [],
   "source": [
    "rec_data.distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1741770863401,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "8ua-XzhqdIr1",
    "outputId": "0d0c5614-cb41-47b6-d2a4-0ac2dc0d6518"
   },
   "outputs": [],
   "source": [
    "rec_data.plot_porosity_distribution(porosity=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1741770682983,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "TTV6dBECdNoH",
    "outputId": "d49e5038-81b2-4ba2-fd32-7e372cca77d8"
   },
   "outputs": [],
   "source": [
    "        df = rec_data.as_dataframe(porosity=0)\n",
    "        fig = px.histogram(df.iloc[:,3], facet_col='variable', title=f\"Porosity Histogram (Density: {rec_data.density})\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_data.as_dataframe(porosity=0)\n",
    "fig = px.histogram(df.iloc[:,3], facet_col='variable', title=f\"Porosity Histogram (Density: {original_data.density})\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1741770683199,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "Aytt3kZYdTgr",
    "outputId": "4bac8504-8a0a-4340-c3fd-32eec706482a"
   },
   "outputs": [],
   "source": [
    "plt.plot(rec_data.distribution[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1741770683747,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "Pj9gp97cdt_Y",
    "outputId": "b563d994-ea5d-4c2b-ade5-d733f03cd909"
   },
   "outputs": [],
   "source": [
    "plt.plot(rec_data.distribution[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 582,
     "status": "ok",
     "timestamp": 1741768062348,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "9pLlr8H0dy1F",
    "outputId": "55592506-b163-44ee-ab6d-c050ecda558c"
   },
   "outputs": [],
   "source": [
    "px.histogram(rec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "error",
     "timestamp": 1741768095468,
     "user": {
      "displayName": "Pietro del Sorbo",
      "userId": "01555363494258854793"
     },
     "user_tz": -60
    },
    "id": "n-Q2nTIXd3G1",
    "outputId": "bbe3d59a-8545-4e49-dd2d-66b7278e91c2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGLOdL5vd_TP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPu6mAXAX36tAri1fom+JvV",
   "provenance": [
    {
     "file_id": "1Kc2aZZvLQha0Y4R3eNMnC0oT6dTpNpIz",
     "timestamp": 1741691477194
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
